{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMg9IBD+14FL2ztOC4DprK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnsbrdbr/SimCLR/blob/master/SimCLR/SimCLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2IZ-CgTHJxIC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import shutil, time, os, requests, random, copy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=16):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n"
      ],
      "metadata": {
        "id": "EnamaP4VKe_c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download CIFAR10 Dataset\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "\n",
        "!tar -xf /content/cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBpZI9THLrJt",
        "outputId": "7de89f3c-8a29-40a8-e71c-5984ef3a730e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-24 12:13:33--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  45.5MB/s    in 4.0s    \n",
            "\n",
            "2023-04-24 12:13:37 (40.8 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deserialize the contents of the specified file into a Python dictionary\n",
        "import pickle\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "#This code appears loading and concatenating data from multiple files in the CIFAR-10 dataset,\n",
        "#which is a widely-used benchmark dataset in computer vision\n",
        "#load data\n",
        "train_file=['data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5']#training data\n",
        "images = np.array([],dtype=np.uint8).reshape((0,3072)) #store the training images.\n",
        "labels = np.array([]) #class labels\n",
        "for tf in train_file:\n",
        "  data_dict = unpickle('/content/cifar-10-batches-py/'+tf)\n",
        "  data = data_dict[b'data']\n",
        "  images = np.append(images,data,axis=0)\n",
        "  labels = np.append(labels,data_dict[b'labels'])\n",
        "print(images.shape, labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVTZHkTWRGBu",
        "outputId": "a94de251-b66b-411a-9872-cc4c95db5add"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3072) (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testimages = np.array([],dtype=np.uint8).reshape((0,3072))\n",
        "testlabels = np.array([])\n",
        "data_dict = unpickle('/content/cifar-10-batches-py/test_batch')\n",
        "data = data_dict[b'data']\n",
        "testimages = np.append(testimages,data,axis=0)\n",
        "testlabels = np.append(testlabels,data_dict[b'labels'])\n",
        "print(testimages.shape, testlabels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQLlGqaWqjIr",
        "outputId": "336cbc71-728a-4f27-eb36-4d9ae117a514"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 3072) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = images.reshape((-1,3,32,32)).astype(np.float)\n",
        "testimages = testimages.reshape((-1,3,32,32)).astype(np.float)\n",
        "\n",
        "lab_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',6:'frog',7:'horse',8:'ship',9:'truck'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccb25OPnrxEj",
        "outputId": "17ac7ecd-fb33-4c62-c61a-58a6bb8b566d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a2b46abd833f>:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  images = images.reshape((-1,3,32,32)).astype(np.float)\n",
            "<ipython-input-6-a2b46abd833f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  testimages = testimages.reshape((-1,3,32,32)).astype(np.float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trimages = images[:40000]\n",
        "valimages = images[40000:]\n",
        "trlabels = labels[:40000]\n",
        "vallabels = labels[40000:]\n",
        "MEAN = np.mean(trimages/255.0,axis=(0,2,3),keepdims=True)\n",
        "STD = np.std(trimages/255.0,axis=(0,2,3),keepdims=True)"
      ],
      "metadata": {
        "id": "eEqllN1h6OD2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class C10DataGen(Dataset):\n",
        "    def __init__(self,phase,imgarr,s = 0.5):\n",
        "        self.phase = phase\n",
        "        self.imgarr = imgarr\n",
        "        self.s = s\n",
        "        self.transforms = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n",
        "                                              transforms.RandomResizedCrop(32,(0.8,1.0)),\n",
        "                                              transforms.Compose([transforms.RandomApply([transforms.ColorJitter(0.8*self.s, \n",
        "                                                                                                                 0.8*self.s, \n",
        "                                                                                                                 0.8*self.s, \n",
        "                                                                                                                 0.2*self.s)], p = 0.8),\n",
        "                                                                  transforms.RandomGrayscale(p=0.2)\n",
        "                                                                 ])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgarr.shape[0]\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        \n",
        "        x = self.imgarr[idx] \n",
        "        #print(x.shape)\n",
        "        x = x.astype(np.float32)/255.0\n",
        "\n",
        "        x1 = self.augment(torch.from_numpy(x))\n",
        "        x2 = self.augment(torch.from_numpy(x))\n",
        "        \n",
        "        x1 = self.preprocess(x1)\n",
        "        x2 = self.preprocess(x2)\n",
        "        \n",
        "        return x1, x2\n",
        "\n",
        "    #shuffles the dataset at the end of each epoch\n",
        "    def on_epoch_end(self):\n",
        "        self.imgarr = self.imgarr[random.sample(population = list(range(self.__len__())),k = self.__len__())]\n",
        "\n",
        "    def preprocess(self,frame):\n",
        "        frame = (frame-MEAN)/STD\n",
        "        return frame\n",
        "    \n",
        "    #applies randomly selected augmentations to each clip (same for each frame in the clip)\n",
        "    def augment(self, frame, transformations = None):\n",
        "        \n",
        "        if self.phase == 'train':\n",
        "            frame = self.transforms(frame)\n",
        "        else:\n",
        "            return frame\n",
        "        \n",
        "        return frame\n"
      ],
      "metadata": {
        "id": "4fuu7Ysmry_-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dg = C10DataGen('train',trimages)\n",
        "train_loader = DataLoader(dg,batch_size = 128,drop_last=True)\n",
        "\n",
        "vdg = C10DataGen('valid',valimages)\n",
        "valid_loader = DataLoader(vdg,batch_size = 128,drop_last=True)\n"
      ],
      "metadata": {
        "id": "U0DQ9nRJ30eB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 out_features,\n",
        "                 use_bias = True,\n",
        "                 use_bn = False,\n",
        "                 **kwargs):\n",
        "        super(LinearLayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "        self.use_bn = use_bn\n",
        "        \n",
        "        self.linear = nn.Linear(self.in_features, \n",
        "                                self.out_features, \n",
        "                                bias = self.use_bias and not self.use_bn)\n",
        "        if self.use_bn:\n",
        "             self.bn = nn.BatchNorm1d(self.out_features)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.linear(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features,\n",
        "                 out_features,\n",
        "                 head_type = 'nonlinear',\n",
        "                 **kwargs):\n",
        "        super(ProjectionHead,self).__init__(**kwargs)\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.hidden_features = hidden_features\n",
        "        self.head_type = head_type\n",
        "\n",
        "        if self.head_type == 'linear':\n",
        "            self.layers = LinearLayer(self.in_features,self.out_features,False, True)\n",
        "        elif self.head_type == 'nonlinear':\n",
        "            self.layers = nn.Sequential(\n",
        "                LinearLayer(self.in_features,self.hidden_features,True, True),\n",
        "                nn.ReLU(),\n",
        "                LinearLayer(self.hidden_features,self.out_features,False,True))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "class PreModel(nn.Module):\n",
        "    def __init__(self,base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        \n",
        "        #PRETRAINED MODEL\n",
        "        self.pretrained = models.resnet50(pretrained=True)\n",
        "        \n",
        "        self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
        "        self.pretrained.maxpool = Identity()\n",
        "        \n",
        "        self.pretrained.fc = Identity()\n",
        "        \n",
        "        for p in self.pretrained.parameters():\n",
        "            p.requires_grad = True\n",
        "        \n",
        "        self.projector = ProjectionHead(2048, 2048, 128)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.pretrained(x)\n",
        "        \n",
        "        xp = self.projector(torch.squeeze(out))\n",
        "        \n",
        "        return xp"
      ],
      "metadata": {
        "id": "MCrb_q036Vq7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PreModel('resnet50')"
      ],
      "metadata": {
        "id": "vjysrJOH6uYH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR_Loss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "\n",
        "        N = 2 * self.batch_size\n",
        "\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "\n",
        "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        sim_i_j = torch.diag(sim, self.batch_size)\n",
        "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
        "        \n",
        "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
        "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
        "        negative_samples = sim[self.mask].reshape(N, -1)\n",
        "        \n",
        "        #SIMCLR\n",
        "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
        "        \n",
        "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        \n",
        "        return loss"
      ],
      "metadata": {
        "id": "3CzoQQ9F643D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "import re\n",
        "\n",
        "EETA_DEFAULT = 0.001\n",
        "\n",
        "\n",
        "class LARS(Optimizer):\n",
        "    \"\"\"\n",
        "    Layer-wise Adaptive Rate Scaling for large batch training.\n",
        "    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
        "    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr=required,\n",
        "        momentum=0.9,\n",
        "        use_nesterov=False,\n",
        "        weight_decay=0.0,\n",
        "        exclude_from_weight_decay=None,\n",
        "        exclude_from_layer_adaptation=None,\n",
        "        classic_momentum=True,\n",
        "        eeta=EETA_DEFAULT,\n",
        "    ):\n",
        "        \"\"\"Constructs a LARSOptimizer.\n",
        "        Args:\n",
        "        lr: A `float` for learning rate.\n",
        "        momentum: A `float` for momentum.\n",
        "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
        "        weight_decay: A `float` for weight decay.\n",
        "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
        "            any of the string appears in a variable's name, the variable will be\n",
        "            excluded for computing weight decay. For example, one could specify\n",
        "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
        "            from weight decay.\n",
        "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
        "            for layer adaptation. If it is None, it will be defaulted the same as\n",
        "            exclude_from_weight_decay.\n",
        "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
        "            momentum. The learning rate is applied during momeuntum update in\n",
        "            classic momentum, but after momentum for popular momentum.\n",
        "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
        "        name: The name for the scope.\n",
        "        \"\"\"\n",
        "\n",
        "        self.epoch = 0\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            use_nesterov=use_nesterov,\n",
        "            weight_decay=weight_decay,\n",
        "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
        "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
        "            classic_momentum=classic_momentum,\n",
        "            eeta=eeta,\n",
        "        )\n",
        "\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.use_nesterov = use_nesterov\n",
        "        self.classic_momentum = classic_momentum\n",
        "        self.eeta = eeta\n",
        "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
        "        # arg is None.\n",
        "        if exclude_from_layer_adaptation:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "        else:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "    def step(self, epoch=None, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        if epoch is None:\n",
        "            epoch = self.epoch\n",
        "            self.epoch += 1\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            momentum = group[\"momentum\"]\n",
        "            eeta = group[\"eeta\"]\n",
        "            lr = group[\"lr\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param = p.data\n",
        "                grad = p.grad.data\n",
        "\n",
        "                param_state = self.state[p]\n",
        "\n",
        "                # TODO: get param names\n",
        "                # if self._use_weight_decay(param_name):\n",
        "                grad += self.weight_decay * param\n",
        "\n",
        "                if self.classic_momentum:\n",
        "                    trust_ratio = 1.0\n",
        "\n",
        "                    # TODO: get param names\n",
        "                    # if self._do_layer_adaptation(param_name):\n",
        "                    w_norm = torch.norm(param)\n",
        "                    g_norm = torch.norm(grad)\n",
        "\n",
        "                    device = g_norm.get_device()\n",
        "                    trust_ratio = torch.where(\n",
        "                        w_norm.gt(0),\n",
        "                        torch.where(\n",
        "                            g_norm.gt(0),\n",
        "                            (self.eeta * w_norm / g_norm),\n",
        "                            torch.Tensor([1.0]).to(device),\n",
        "                        ),\n",
        "                        torch.Tensor([1.0]).to(device),\n",
        "                    ).item()\n",
        "\n",
        "                    scaled_lr = lr * trust_ratio\n",
        "                    if \"momentum_buffer\" not in param_state:\n",
        "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
        "                            p.data\n",
        "                        )\n",
        "                    else:\n",
        "                        next_v = param_state[\"momentum_buffer\"]\n",
        "\n",
        "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
        "                    if self.use_nesterov:\n",
        "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
        "                    else:\n",
        "                        update = next_v\n",
        "\n",
        "                    p.data.add_(-update)\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _use_weight_decay(self, param_name):\n",
        "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "        if not self.weight_decay:\n",
        "            return False\n",
        "        if self.exclude_from_weight_decay:\n",
        "            for r in self.exclude_from_weight_decay:\n",
        "                if re.search(r, param_name) is not None:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def _do_layer_adaptation(self, param_name):\n",
        "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
        "        if self.exclude_from_layer_adaptation:\n",
        "            for r in self.exclude_from_layer_adaptation:\n",
        "                if re.search(r, param_name) is not None:\n",
        "                    return False\n",
        "        return True"
      ],
      "metadata": {
        "id": "6M-m6MKY6_KH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTMIZER\n",
        "optimizer = LARS(\n",
        "    [params for params in model.parameters() if params.requires_grad],\n",
        "    lr=0.2,\n",
        "    weight_decay=1e-6,\n",
        "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
        ")\n",
        "\n",
        "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
        "#SCHEDULER OR LINEAR EWARMUP\n",
        "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
        "\n",
        "#SCHEDULER FOR COSINE DECAY\n",
        "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)\n",
        "\n",
        "#LOSS FUNCTION\n",
        "criterion = SimCLR_Loss(batch_size = 128, temperature = 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhtPkRgo7DTk",
        "outputId": "2c5de3fa-a5e1-48fb-c36c-2dc4b5bb9c0d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-02.\n",
            "Epoch 00000: adjusting learning rate of group 0 to 2.0000e-01.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
        "    out = os.path.join('/content/saved_models/',name.format(current_epoch))\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict':scheduler.state_dict()}, out)\n",
        "\n",
        "def plot_features(model, num_classes, num_feats, batch_size):\n",
        "    preds = np.array([]).reshape((0,1))\n",
        "    gt = np.array([]).reshape((0,1))\n",
        "    feats = np.array([]).reshape((0,num_feats))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x1,x2 in valid_loader:\n",
        "            x1 = x1.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
        "            out = model(x1)\n",
        "            out = out.cpu().data.numpy()#.reshape((1,-1))\n",
        "            feats = np.append(feats,out,axis = 0)\n",
        "    \n",
        "    tsne = TSNE(n_components = 2, perplexity = 50)\n",
        "    x_feats = tsne.fit_transform(feats)\n",
        "    num_samples = int(batch_size*(valimages.shape[0]//batch_size))#(len(val_df)\n",
        "    \n",
        "    for i in range(num_classes):\n",
        "        plt.scatter(x_feats[vallabels[:num_samples]==i,1],x_feats[vallabels[:num_samples]==i,0])\n",
        "    \n",
        "    plt.legend([str(i) for i in range(num_classes)])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iBLhLNL97bUB"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}